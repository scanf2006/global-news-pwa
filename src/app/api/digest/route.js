import { NextResponse } from 'next/server';
import { NewsAggregator } from '@/services/newsAggregator';
import { APICache } from '@/lib/cache';

export async function GET(request) {
    try {
        // Retrieve custom keys from headers first
        const reqHeaders = new Headers(request.headers);
        const userApiKey = reqHeaders.get('x-user-openai-key');
        const userBaseUrl = reqHeaders.get('x-user-openai-base');
        const userModel = reqHeaders.get('x-user-openai-model');
        const skipCache = reqHeaders.get('cache-control') === 'no-cache';

        const apiKey = userApiKey || process.env.OPENAI_API_KEY;
        const baseUrl = userBaseUrl || process.env.OPENAI_API_BASE_URL || 'https://api.openai.com/v1';
        const modelName = userModel || 'gpt-3.5-turbo';

        // 1. Check if we have a cached digest (cache for 4 hours to save tokens)
        // Differentiate cache key if the user provided their own API key, so they can test their own generations instantly
        const cacheKey = userApiKey ? `ai_digest_v1_custom_${userApiKey.slice(-4)}` : 'ai_digest_v1';

        if (!skipCache) {
            const cachedDigest = APICache.get(cacheKey);
            if (cachedDigest) {
                return NextResponse.json({ success: true, data: cachedDigest });
            }
        }

        // Graceful degradation if no API key is set
        if (!apiKey) {
            const fallbackData = {
                content: "### ðŸ¤– AI ç®€æŠ¥æš‚æœªå¼€å¯\n\nç³»ç»Ÿæ£€æµ‹åˆ°æœªé…ç½®å¤§æ¨¡åž‹ API Keyã€‚æ‚¨å¯ä»¥éšæ—¶ç‚¹å‡»ç½‘é¡µä¸Šæ–¹æˆ–çŽ¯å¢ƒå˜é‡ä¸­é…ç½® `OPENAI_API_KEY` æ¥æ¿€æ´»è‡ªåŠ¨ç”Ÿæˆå…¨çƒæ–°é—»æ‘˜è¦çš„è¶…èƒ½åŠ›ã€‚\n\n*æç¤º: æ”¯æŒ OpenAIã€DeepSeek åŠå…¶ä»–å…¼å®¹æ ¼å¼çš„æ¨¡åž‹ã€‚*",
                timestamp: new Date().toISOString(),
                isMock: true
            };
            return NextResponse.json({ success: true, data: fallbackData });
        }

        // 2. Fetch all news to summarize
        const allNews = await NewsAggregator.fetchAllNews();
        if (!allNews || allNews.length === 0) {
            return NextResponse.json({ success: false, error: 'No news available to summarize' }, { status: 404 });
        }

        // 3. Clean and isolate top news
        const topNewsBySource = {};
        allNews.forEach(item => {
            const source = item.source || 'Other';
            if (!topNewsBySource[source]) topNewsBySource[source] = [];
            if (topNewsBySource[source].length < 3) { // Only take top 3 from each
                topNewsBySource[source].push(item.titleTranslated || item.titleOriginal);
            }
        });

        // 4. Build Prompt
        let promptStr = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€å®¢è§‚çš„å…¨çƒæ–°é—»ç¼–è¾‘ã€‚è¯·å°†ä»¥ä¸‹æŒ‰æ•°æ®æºåˆ†ç±»çš„çƒ­ç‚¹æ–°é—»ï¼Œè¿›è¡Œç»¼åˆåˆ†æžå¹¶æ€»ç»“æˆä¸€ä»½ç»“æž„åŒ–çš„é«˜è´¨é‡æ—©/æ™šæŠ¥ï¼ˆå»ºè®®ç¯‡å¹…åœ¨300-500å­—ï¼‰ã€‚\n\n";
        promptStr += "è¦æ±‚ï¼š\n1. ç»“æž„æ¸…æ™°ï¼Œå¯ä½¿ç”¨ç²—ä½“å’Œemojiã€‚\n2. æç‚¼å‡ºè·¨å¹³å°è¢«åŒæ—¶è®¨è®ºçš„'ç„¦ç‚¹äº‹ä»¶'ï¼ŒåŽ»é‡åŽ»é—²èŠã€‚\n3. ç›´æŽ¥è¾“å‡ºMarkdownæ ¼å¼æ­£æ–‡ï¼Œä¸è¦æœ‰ä»»ä½•å¤šä½™çš„å¯’æš„ã€‚\n\næ–°é—»æ•°æ®ï¼š\n";

        for (const [source, titles] of Object.entries(topNewsBySource)) {
            promptStr += `ã€${source}ã€‘\n- ${titles.join('\n- ')}\n\n`;
        }

        // 5. Call LLM API (OpenAI compatible)
        const response = await fetch(`${baseUrl}/chat/completions`, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${apiKey}`
            },
            body: JSON.stringify({
                model: modelName,
                messages: [
                    { role: 'system', content: 'You are an objective news editor.' },
                    { role: 'user', content: promptStr }
                ],
                temperature: 0.5
            })
        });

        if (!response.ok) {
            const errText = await response.text();
            console.error('LLM API Error:', errText);
            return NextResponse.json({ success: false, error: 'LLM API failed' }, { status: 502 });
        }

        const completion = await response.json();
        let digestContent = completion.choices?.[0]?.message?.content || 'ç”Ÿæˆç®€æŠ¥å¤±è´¥ã€‚';
        const finishReason = completion.choices?.[0]?.finish_reason || 'unknown';

        if (finishReason !== 'stop' && finishReason !== 'unknown') {
            digestContent += `\n\n*(Debug: æ‘˜è¦è¢«æ„å¤–æˆªæ–­ã€‚Finish Reason: ${finishReason})*`;
        }

        const digestData = {
            content: digestContent,
            timestamp: new Date().toISOString(),
            isMock: false
        };

        // Cache for 4 hours (14400 seconds)
        APICache.set(cacheKey, digestData, 14400);

        return NextResponse.json({ success: true, data: digestData });

    } catch (error) {
        console.error('Digest API Exception:', error);
        return NextResponse.json({ success: false, error: 'Internal server error' }, { status: 500 });
    }
}
